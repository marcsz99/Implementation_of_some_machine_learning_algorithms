{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868926be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf62f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_dist(a,b):\n",
    "    \"\"\"\n",
    "    Takes two vectors a and b of equal length.\n",
    "    Outputs the Euclidean distance between a and b. \n",
    "    \"\"\"\n",
    "    \n",
    "    sq_dist = 0\n",
    "    l = a.size\n",
    "    \n",
    "    for i in range(l):\n",
    "        sq_dist += (a[i] - b[i])**2\n",
    "    \n",
    "    e_dist = np.sqrt(sq_dist)\n",
    "    return e_dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2249129",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([2,1,1])\n",
    "b = np.array([1,1,1])\n",
    "print(euclid_dist(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11fe1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(sample, xtrain, ytrain, n):\n",
    "    \"\"\"\n",
    "     Input\n",
    "\n",
    "     Sample: a 1*c array where c is equal to the number of features in the given dataset.\n",
    "     xtrain: a d*c array where d is equal to the number of training samples.\n",
    "     ytrain: a 1*d array containg all the lables for xtrain.\n",
    "     n: The number of nearest neighbours that predictions are made from\n",
    "     \n",
    "     Output\n",
    "     \n",
    "     The predicted label for sample using the knn method given the training dataset and n.\n",
    "\n",
    "    \"\"\"\n",
    "    ###  Calculate euclidian distance between sample and all samples in xtrain.\n",
    "\n",
    "    l = np.size(ytrain)\n",
    "    euclid = np.zeros(l)\n",
    "    for i in range(l):\n",
    "        euclid[i] = euclid_dist(sample,xtrain[i])\n",
    "        \n",
    "    ###  Check if n = 1. If n = 1 then unnecessary code my be bypassed increasing computational effciency.\n",
    "    if n == 1:\n",
    "        index_min = np.argmin(euclid)\n",
    "        pred = ytrain[index_min]\n",
    "    else:\n",
    "        ###  Compute the neareat n lables to sample and output most common as prediction.\n",
    "        near_lab = np.zeros(n,dtype = int)\n",
    "        \n",
    "        for j in range(n):\n",
    "            #Find min euclid distance, then find its label, then min euclid distance with inf. Reapeat process to find n \n",
    "            #nearest lables. \n",
    "            near_lab_index = np.argmin(euclid)\n",
    "            near_lab[j] = ytrain[near_lab_index]\n",
    "            euclid[near_lab_index] = math.inf\n",
    "        \n",
    "        ### Now find the mode of nearest labels to obtain prediction\n",
    "        vals, counts = np.unique(near_lab, return_counts=True)\n",
    "        mode_value = np.argwhere(counts == np.max(counts))\n",
    "        mode = vals[mode_value]\n",
    "        \n",
    "        ### Sometimes there may be more than 1 modes. This if else statments deals with this by assigning a mode randomly to \n",
    "        ### predict if there are.\n",
    "        \n",
    "        if np.size(mode) == 1:\n",
    "            pred = mode\n",
    "        else:\n",
    "            rand = np.random.randint(np.size(mode))\n",
    "            pred = np.array([mode[rand]])\n",
    "        ###Extract single value from array \n",
    "        pred = pred[0,0]\n",
    "    \n",
    "    return pred \n",
    "        \n",
    "        \n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a19e4f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_predictor(sample, xtrain, ytrain):\n",
    "    \"\"\"\n",
    "     Input\n",
    "\n",
    "     Sample: a 1*c array where c is equal to the number of features in the given dataset.\n",
    "     xtrain: a d*c array where d is equal to the number of training samples.\n",
    "     ytrain: a 1*d array containg all the lables for xtrain.\n",
    "     \n",
    "     Output \n",
    "     \n",
    "     p-values and averge false p-values\n",
    "    \"\"\"\n",
    "    ### Concatenate sample and xtrain\n",
    "    full_sample = xtrain.copy()\n",
    "    full_sample = np.append(xtrain, [sample], axis = 0)\n",
    "    ###Find all the unique labels \n",
    "    uniq_lab = np.unique(ytrain)\n",
    "    \n",
    "    ###Array to store p-values \n",
    "    p_value = np.zeros(uniq_lab.size)\n",
    "    \n",
    "    ###Variable to store label index\n",
    "    label_index = 0 \n",
    "    \n",
    "    for label in uniq_lab:\n",
    "        full_label = ytrain.copy()\n",
    "        full_label = np.append(full_label,label)\n",
    "        full_label = np.array([full_label])\n",
    "        full_s_l = np.concatenate((full_sample,full_label.T),axis = 1)\n",
    "        conformity_score = np.zeros(full_label.size)\n",
    "        #### find distances\n",
    "        for i in range(full_label.size):\n",
    "            current_sample = full_sample[i]\n",
    "            current_label = full_label[0,i]\n",
    "        ### distance to neareast sample where class is diffrent to current label\n",
    "        ### first find all samples that are of a different class\n",
    "        \n",
    "            diff_full_s_l = full_s_l[full_s_l[:,-1] != current_label]\n",
    "            samples_different = diff_full_s_l[:,:-1]\n",
    "            \n",
    "        ### compute all ditances between cuurent sample and samples of a different class \n",
    "            dist_diff = np.zeros(samples_different.shape[0])\n",
    "            for j in range(samples_different.shape[0]):\n",
    "                dist_diff[j] = euclid_dist(current_sample, samples_different[j])\n",
    "            \n",
    "        ### find distance of closest sample of a different class \n",
    "            close_diff = np.min(dist_diff)\n",
    "            \n",
    "        ### Now find all the samples of the same class as current label\n",
    "            same_full_s_l = full_s_l[full_s_l[:,-1] == current_label]\n",
    "            samples_same = same_full_s_l[:,:-1]\n",
    "            \n",
    "        ### compute all distances between current sample and samples of the same class  \n",
    "            dist_same = np.zeros(samples_same.shape[0])\n",
    "            for z in range(samples_same.shape[0]):\n",
    "                dist_same[z] = euclid_dist(current_sample, samples_same[z])\n",
    "            \n",
    "        \n",
    "        ### find distance of closest sample of same class \n",
    "            close_same = np.min(dist_same[dist_same != 0])\n",
    "            \n",
    "        ### with distance to closest sample of nearest and furthest class now calculated the corresponding conformity score\n",
    "        ### can be calculated \n",
    "            conformity_score[i] = close_diff/close_same\n",
    "        \n",
    "    ### Now calculate p value for the postulated label\n",
    "        rank = 0\n",
    "        for n in range(conformity_score.size):\n",
    "            if conformity_score[-1] >= conformity_score[n]:\n",
    "                rank = rank + 1\n",
    "        p_value[label_index] = rank/conformity_score.size\n",
    "        #print('The current label is ' + str(label))\n",
    "        #print('The p_values is: ' + str(p_value[label_index]))\n",
    "        label_index += 1 \n",
    "        \n",
    "    return (list(p_value))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c39bc1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted label for x based on 1NN is:  1\n",
      "The p-values are:  [0.14285714285714285, 0.8571428571428571]\n"
     ]
    }
   ],
   "source": [
    "###testing conformal preictor and knn on lecture example.\n",
    "\n",
    "x_train = np.array([[0,3],[2,2],[3,3],[-1,1],[-1,-1],[0,1]])\n",
    "y_train = np.array([1,1,1,-1,-1,-1])\n",
    "s = np.array([2,3])\n",
    "\n",
    "print('The predicted label for x based on 1NN is: ', knn(s, x_train, y_train, 1))\n",
    "\n",
    "print('The p-values are: ' ,conformal_predictor(s,x_train,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fd53d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(38, 4)\n",
      "(112,)\n",
      "(38,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris()\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=211)\n",
    "\n",
    "##Get the shapes of X_train, X_test, y_train and y_test. This will be useful information in latter analysis \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea34138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate for the test set is 0.0.\n"
     ]
    }
   ],
   "source": [
    "#Error rate for 1 nearsest neighbour\n",
    "prediction = np.zeros(X_test.shape[0],dtype = int)\n",
    "for i in range(X_test.shape[0]):\n",
    "    prediction[i] = knn(X_test[i], X_train, y_train, 1)\n",
    "\n",
    "error_rate =  1 - np.mean(prediction == y_test)\n",
    "\n",
    "print('The error rate for the test set is ' + str(error_rate) + '.')\n",
    "##Error rate given as 0% suggesting that 1NN is good algorithm for clasifying different IRIS types. Although the true test \n",
    "## error rate is likely higher than 0.0 as the test set used was relativally small cointaing only 38 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3757e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02631579 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "#Error rates up to 10 nearest neighbours \n",
    "error_rate2 = np.zeros(10)\n",
    "    \n",
    "for j in range(error_rate2.size):\n",
    "    \n",
    "    prediction2 = np.zeros(X_test.shape[0],dtype = int)\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        prediction2[i] = knn(X_test[i], X_train, y_train, j+1)\n",
    "\n",
    "    error_rate2[j] =  1 - np.mean(prediction2 == y_test)\n",
    "\n",
    "print(error_rate2)\n",
    "#Given the error rate is zero for 1NN I don't expect much difference for K > 1. The results seem to enforce my expectation with \n",
    "#all error rates = 0, with the expecation of K = 8 containing only 1 misprediction as 1/38 = 0.0263. \n",
    "#A test set is needed to gain a better understanding of the optimal K to to fit the model with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13fdd959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.729015350341797 seconds\n"
     ]
    }
   ],
   "source": [
    "#get p-values for 1NN conformal predictor for every sample in the X_test set.\n",
    "import time \n",
    "start = time.time()\n",
    "\n",
    "p_vals = list(np.zeros([X_test.shape[0]]))\n",
    "\n",
    "for i in range(len(p_vals)):\n",
    "    p_vals[i] = conformal_predictor(X_test[i], X_train, y_train)\n",
    "\n",
    "print(time.time() - start, 'seconds')\n",
    "#As can be seen here the conformal predictor for 1NN takes a relatively long time to run over the the whole of x_train \n",
    "#so effecinecy improvements are likely possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbaefb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.008849557522123894, 0.46017699115044247, 0.008849557522123894], [0.017699115044247787, 0.017699115044247787, 0.10619469026548672], [0.8053097345132744, 0.008849557522123894, 0.008849557522123894], [0.6902654867256637, 0.008849557522123894, 0.008849557522123894], [0.008849557522123894, 0.6460176991150443, 0.008849557522123894], [0.7079646017699115, 0.008849557522123894, 0.008849557522123894], [0.008849557522123894, 0.3008849557522124, 0.017699115044247787], [0.008849557522123894, 0.24778761061946902, 0.017699115044247787], [0.008849557522123894, 0.008849557522123894, 0.7168141592920354], [0.008849557522123894, 0.008849557522123894, 0.49557522123893805], [0.8495575221238938, 0.008849557522123894, 0.008849557522123894], [0.6902654867256637, 0.008849557522123894, 0.008849557522123894], [0.008849557522123894, 0.008849557522123894, 0.7522123893805309], [0.008849557522123894, 0.008849557522123894, 0.45132743362831856], [0.008849557522123894, 0.46017699115044247, 0.008849557522123894], [0.008849557522123894, 0.008849557522123894, 0.5486725663716814], [0.008849557522123894, 0.36283185840707965, 0.008849557522123894], [0.008849557522123894, 0.672566371681416, 0.008849557522123894], [0.008849557522123894, 0.008849557522123894, 0.37168141592920356], [0.008849557522123894, 0.008849557522123894, 0.45132743362831856], [0.008849557522123894, 0.18584070796460178, 0.017699115044247787], [0.008849557522123894, 0.7079646017699115, 0.008849557522123894], [0.008849557522123894, 0.19469026548672566, 0.017699115044247787], [0.9557522123893806, 0.008849557522123894, 0.008849557522123894], [0.008849557522123894, 0.07964601769911504, 0.035398230088495575], [0.008849557522123894, 0.008849557522123894, 0.415929203539823], [0.008849557522123894, 0.4424778761061947, 0.008849557522123894], [0.008849557522123894, 0.23893805309734514, 0.02654867256637168], [0.9646017699115044, 0.008849557522123894, 0.008849557522123894], [0.008849557522123894, 0.8053097345132744, 0.008849557522123894], [0.6814159292035398, 0.008849557522123894, 0.008849557522123894], [0.008849557522123894, 0.4690265486725664, 0.008849557522123894], [0.008849557522123894, 0.008849557522123894, 0.415929203539823], [0.008849557522123894, 0.46017699115044247, 0.008849557522123894], [0.008849557522123894, 0.008849557522123894, 0.35398230088495575], [0.008849557522123894, 0.672566371681416, 0.008849557522123894], [0.008849557522123894, 0.008849557522123894, 0.4247787610619469], [0.6548672566371682, 0.008849557522123894, 0.008849557522123894]]\n"
     ]
    }
   ],
   "source": [
    "### Here the p_values for all possible labels for every sample in X_test is printed.   \n",
    "\n",
    "print(p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2e34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average false p_value is 0.010130414531904984.\n"
     ]
    }
   ],
   "source": [
    "#Get average false p-value from the p value calculated in the above cells.\n",
    "unique_labels = np.unique(y_train)\n",
    "false_p_vals = np.zeros(y_test.size)\n",
    "\n",
    "for i in range(y_test.size):\n",
    "    # Find the label of the sample and then find the p-values which don't have the particular label.\n",
    "    lab_of_index = y_test[i]\n",
    "    samp_of_index_p = p_vals[i]\n",
    "    diff_lab = unique_labels[unique_labels != lab_of_index]\n",
    "    not_lab_p1 = samp_of_index_p[diff_lab[0]]\n",
    "    not_lab_p2 = samp_of_index_p[diff_lab[1]]\n",
    "    false_p_vals[i] = (not_lab_p1 + not_lab_p2)/2\n",
    "\n",
    "# Find the avergae of all the false p values \n",
    "avg_false_p_value = np.mean(false_p_vals)\n",
    "\n",
    "print('The average false p_value is ' + str(avg_false_p_value) + '.')\n",
    "\n",
    "#The average false p_value is given as 0.0101304. The lowest possible false p value, where all 'false' labels are least \n",
    "#conforming is 1/(n + 1) = 1/(112 + 1) = 0.008850 where n is the sample size of xtrain. Give the average false p value is \n",
    "#near to the lowest possible p value we consider the model to fit the test data very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "453a564d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 35)\n",
      "(88, 35)\n",
      "(263,)\n",
      "(88,)\n"
     ]
    }
   ],
   "source": [
    "#load in the ionosphere data and split\n",
    "X = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\")\n",
    "y = np.genfromtxt(\"ionosphere.txt\", delimiter=\",\",\n",
    "usecols=34, dtype='int')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 211)\n",
    "\n",
    "##Get the shapes of X_train, X_test, y_train and y_test. This will be useful information in latter analysis \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "129b647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate for the test set is 0.03409090909090906.\n"
     ]
    }
   ],
   "source": [
    "#Error rate for 1 nearest neighbour\n",
    "prediction = np.zeros(X_test.shape[0],dtype = int)\n",
    "for i in range(X_test.shape[0]):\n",
    "    prediction[i] = knn(X_test[i], X_train, y_train, 1)\n",
    "\n",
    "error_rate =  1 - np.mean(prediction == y_test)\n",
    "\n",
    "print('The error rate for the test set is ' + str(error_rate) + '.')\n",
    "### The error rate for the lest set is around 0.03409. This suggests that the model will make incorrect predictions on unseen\n",
    "## data approximatly 3.4% of the time. So, 3 labels were predicted incorrectly as 3/88 = 0.03408.\n",
    "## This suggests the 1NN model is a relativally good fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd4d19a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03409091 0.02272727 0.02272727 0.04545455 0.04545455 0.03409091\n",
      " 0.04545455 0.06818182 0.07954545 0.07954545]\n"
     ]
    }
   ],
   "source": [
    "#Error rates up to 10 nearest neighbours \n",
    "error_rate2 = np.zeros(10)\n",
    "    \n",
    "for j in range(error_rate2.size):\n",
    "    \n",
    "    prediction2 = np.zeros(X_test.shape[0],dtype = int)\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        prediction2[i] = knn(X_test[i], X_train, y_train, j+1)\n",
    "\n",
    "    error_rate2[j] =  1 - np.mean(prediction2 == y_test)\n",
    "\n",
    "print(error_rate2)\n",
    "#Here unlike the iris data set we have p values for all k so an idea of the optimal k can be obtained. This will be done\n",
    "#via a plot in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "704ea9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error rate')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtMUlEQVR4nO3deXwV9b3/8deHhLAvIqtsQUVWZTHs1mr1WkCv2HtrBTdAWmqv3Nr29tervd3sdrvctmprtVYwoCii1UoV97UKaMIiskqEAGFLANkSICT5/P44Q3sMJ3CAnMzJOe/n43EemTPznZlPJsvnzMxnvl9zd0RERKprEHYAIiKSnJQgREQkJiUIERGJSQlCRERiUoIQEZGYMsMOoDa1bdvWs7Ozww5DRKTeWLx48U53bxdrWUoliOzsbPLz88MOQ0Sk3jCzjTUt0yUmERGJSQlCRERiUoIQEZGYlCBERCSmhCYIMxttZmvNrMDM7oix3Mzs3mD5cjMbHLXsm2a20sxWmNnjZtY4kbGKiMinJSxBmFkGcB8wBugLTDCzvtWajQF6Bq+pwP3Bup2BrwM57t4fyADGJypWERE5ViLPIIYCBe6+3t3LgTnAuGptxgGzPGIR0NrMOgXLMoEmZpYJNAW2JjBWERGpJpHPQXQGNke9LwKGxdGms7vnm9n/AZuAg8DL7v5yrJ2Y2VQiZx9069atlkIXETm+4n2HeCJvM0cqq8IOhaaNMrn1s+fU+nYTmSAsxrzqg0/EbGNmZxA5u+gB7AGeNLMb3f3RYxq7Pwg8CJCTk6PBLUQk4fYfOsLNM95nzfb9WKz/YnWsbfNG9S5BFAFdo9534djLRDW1uRzY4O4lAGb2NDASOCZBiIjUpYrKKqY9tpR1xQd4ZMpQPtMzZi8VKSGR9yDygJ5m1sPMsojcZJ5Xrc084Oagmmk4sNfdtxG5tDTczJqamQGXAasTGKuIyAm5O3f9bRVvfVTCT6/pn9LJARJ4BuHuFWY2DXiJSBXSDHdfaWa3BssfAOYDY4ECoAyYHCx7z8yeApYAFcBSgstIIiJhmfFuIY8s2shXLz6bCUNT/56npdKY1Dk5Oa7O+kQkEV5ZtYOpj+Tz+b4d+eMNg2nQIAluPtQCM1vs7jmxlulJahGRE1ixZS9ff3wpF3Ruxe+uG5gyyeFElCBERI5j296DTJmZR5tmWfx5Yg5NsjLCDqnOpNR4ECIitenA4Qpuyc2n9HAlf/naMNq3SK8ef5QgRERiqKis4uuPL+WjHfuZMWkIvTq2CDukOqdLTCIiMfz0+dW8vqaYu67ux2fPS+1y1pooQYiIVJP77gZyFxTy5Yt6cOPw7mGHExolCBGRKK+v2cGPn1vFv/TtwJ1j+4QdTqiUIEREAiu37mXaY0vpd1Yr7hk/kIw0KWetiRKEiAiwfe8hpuTm06pJQx6amEPTLNXwKEGISNorPVzBlJl57D90hBmThtChZXqVs9ZEKVJE0lpllXP7nKWs3raP6ROH0KdTy7BDShpKECKS1n72/GpeXV3Mj8f149Le7cMOJ6noEpOIpK1HFhYy490NTB6Vzc0jssMOJ+koQYhIWnpjbTE/nLeSy/u053tX9g07nKSkBCEiaWf1tn1Mm72EPp1acs/4QWlfzloTJQgRSSvF+w4xJTePFo0bMn3iEJo10q3YmujIiEjaKCuvYMrMfPYcPMKTt46gYyuVsx6PziBEJC1UVjnfmLOMlVv38vsJg+h3VquwQ0p6ShAikhZ+8cJqXl61g+9f1ZfL+nQIO5x6QQlCRFLe7Pc28ue/b2DiiO5MHtUj7HDqDSUIEUlpb31Uwg+eXcmlvdrx/atUznoylCBEJGWt3b6f22Yv4bwOLfj99YPJzNC/vJOhoyUiKal4/yFuyc2jaVYG0yfm0FzlrCdNR0xEUs7B8kq+MjOf3aXlzP3qCM5q3STskOolJQgRSSlVVc635i5j+Za9/OnGCzm/i8pZT1VCLzGZ2WgzW2tmBWZ2R4zlZmb3BsuXm9ngYH4vM1sW9dpnZt9IZKwikhp++dIaXlixnf8Z24cr+nUMO5x6LWFnEGaWAdwH/AtQBOSZ2Tx3XxXVbAzQM3gNA+4Hhrn7WmBg1Ha2AM8kKlYRSQ2Pv7+JP721nhuGdWPKRSpnPV2JPIMYChS4+3p3LwfmAOOqtRkHzPKIRUBrM+tUrc1lwMfuvjGBsYpIPffOup18768ruPi8dtx1dT/M1AHf6UpkgugMbI56XxTMO9k244HHa9qJmU01s3wzyy8pKTmNcEWkvlq3Yz9fm72Yc9s1577rB6mctZYk8ijGSt9+Mm3MLAu4Gniypp24+4PunuPuOe3atTulQEWk/irZf5jJuXk0ysxg+qQcWjRuGHZIKSORCaII6Br1vguw9STbjAGWuPuOhEQoIvXaoSOVfGVWPjsPHGb6xBy6nNE07JBSSiITRB7Q08x6BGcC44F51drMA24OqpmGA3vdfVvU8gkc5/KSiKSvqirnv+Z+wAdFe7j7uoEM6No67JBSTsKqmNy9wsymAS8BGcAMd19pZrcGyx8A5gNjgQKgDJh8dH0za0qkAuqriYpRROqv/3t5Lc9/uI07x/RmdP/qtS1SGxL6oJy7zyeSBKLnPRA17cBtNaxbBpyZyPhEpH6am7+ZP775MROGdmXqxWeHHU7K0q1+EalXPtqxn+8+/SGf6dmWH4/rr3LWBFKCEJF6ZcY7G8jMMO4ZP4iGKmdNKB1dEak3Pikt56/LtvCFQV1o0ywr7HBSnhKEiNQbT+Rv5tCRKiaO7B52KGlBCUJE6oWKyioeWbiREWefSe+OLcMOJy0oQYhIvfDq6mK27DnIxJHZYYeSNpQgRKRemLmgkM6tm3B5n/Zhh5I2lCBEJOmt2b6Phet3cdOI7uqIrw7pSItI0pu5YCONGzZg/JCuJ24stUYJQkSS2p6ycp5ZWsQ1AzvTuqlKW+uSEoSIJLW5/yhtzQ47lLSjBCEiSauyypm1cCPDerShTyeVttY1JQgRSVqvrd5B0ScHmTwqO+xQ0pIShIgkrdwFhZzVqjGX9+kQdihpSQlCRJLSRzv2s+DjXdw0IlulrSHRUReRpJS7oJBGmSptDZMShIgknb1lR3hmyRauGdiZM9Rra2iUIEQk6czN38zBI5UqbQ2ZEoSIJJXKKmfWokKG9mhD37NU2homJQgRSSqvrylm8+6DTNLZQ+iUIEQkqcxcUEinVo25oq9KW8OmBCEiSWPdjv28U7CTG4er19ZkoJ+AiCSNmQsLycpswISh3cIORVCCEJEksffgEf6yeAvjBpxFG5W2JgUlCBFJCk+qtDXpKEGISOiO9to6JPsM+nduFXY4EkhogjCz0Wa21swKzOyOGMvNzO4Nli83s8FRy1qb2VNmtsbMVpvZiETGKiLheXNtMZt2lzFpZI+wQ5EoCUsQZpYB3AeMAfoCE8ysb7VmY4CewWsqcH/UsnuAF929NzAAWJ2oWEUkXLkLCunYsjFX9FNpazJJ5BnEUKDA3de7ezkwBxhXrc04YJZHLAJam1knM2sJXAxMB3D3cnffk8BYRSQkBcX7+fu6ndw0ojsNVdqaVBL50+gMbI56XxTMi6fN2UAJ8LCZLTWzh8ysWaydmNlUM8s3s/ySkpLai15E6sTMBRvJUq+tSSmRCcJizPM422QCg4H73X0QUAoccw8DwN0fdPccd89p167d6cQrInVs36Ej/GVJEVcPOIszmzcKOxypJpEJogiI/kjQBdgaZ5sioMjd3wvmP0UkYYhICnkyv4iy8kr1u5SkEpkg8oCeZtbDzLKA8cC8am3mATcH1UzDgb3uvs3dtwObzaxX0O4yYFUCYxWROlZV5cxaWEhOd5W2JqvMRG3Y3SvMbBrwEpABzHD3lWZ2a7D8AWA+MBYoAMqAyVGb+E9gdpBc1ldbJiL13JsfFbNxVxnfvqLXiRtLKBKWIADcfT6RJBA974GoaQduq2HdZUBOIuMTkfDkLthIh5aNGN2/Y9ihSA1UUyYide7jkgO8/VEJNw5TaWsy009GROrcrAWFZGU0YMIw9dqazJQgRKRO7T90hKcWF3HVgE60VWlrUos7QdT0oJqIyMl4anERpeWVTFa/S0nvhAnCzEaa2SqCvpDMbICZ/THhkYlIyqmqcmYuKGRwt9ac30WlrckunjOI3wGfB3YBuPsHRPpJEhE5KW+tK6FwVxmTRunsoT6I6xKTu2+uNqsyAbGISIrLfbeQ9i0aMUalrfVCPAlis5mNBNzMsszs26jrbRE5SetLDvDWRyXcOFylrfVFPD+lW4k8zNaZSB9JA4H/SGBMIpKCZi3cGCltHarS1voiniepe7n7DdEzzGwU8G5iQhKRVPOP0tYLOtGuhUpb64t4ziB+H+c8EZGY/rK4iAOHK5ioXlvrlRrPIIIxoEcC7czsW1GLWhLpfE9E5IQivbZuZFC31gzo2jrscOQkHO8MIgtoTiSJtIh67QO+mPjQRCQVvL2uhPU7SzXmQz1U4xmEu78FvGVmue6+sQ5jEpEUMnNBIe1aNGJM/05hhyInKZ6b1GVm9mugH9D46Ex3/1zCohKRlLBhZylvrC3hG5f3JCtTpa31TTw/sdnAGqAHcBdQSGS0OBGR45q1sJCGGcb16rW1XoonQZzp7tOBI+7+lrvfAgxPcFwiUs8dOFzBk/lFXHl+J9q3aHziFSTpxHOJ6UjwdZuZXQlsBbokLiQRSQVPL4mUtqrfpforngTxUzNrBfwXkecfWgLfTGhUIlKvVVU5uQsKGdC1NQNV2lpvHTdBmFkG0NPdnwP2ApfWSVQiUq+9U7CT9SWl3H3dwLBDkdNw3HsQ7l4JXF1HsYhIishdUEjb5o0Ye75KW+uzeC4xLTCzPwBPAKVHZ7r7koRFJSL1VuHOUt5YW8zXP6fS1voungQxMvj646h5Dug5CBE5xqyFG8kw4waVttZ7J0wQ7q77DiISl9LDFTyZv5krL+hE+5Yqba3vdP4nIrXm6SVF7FevrSlDCUJEaoV7UNrapRWDVNqaEo6bIMysQTDc6Ckxs9FmttbMCszsjhjLzczuDZYvN7PBUcsKzexDM1tmZvmnGoOI1I13CnbycUkpE0dmY2ZhhyO14ERlrlXAb05lw8EzFPcBY4C+wAQz61ut2RigZ/CaCtxfbfml7j7Q3XNOJQYRqTszFxTStnkWV16g0tZUEc8lppfN7N/t5D8SDAUK3H29u5cDc4Bx1dqMA2Z5xCKgtZnpt0ukntm0q4zX1hRz/dBuNMrUeGKpIp4E8S3gSaDczPaZ2X4z2xfHep2BzVHvi4J58bZxIslpsZlNrWknZjbVzPLNLL+kpCSOsESkts1aWBgpbR3ePexQpBbFU+ba4hS3HeuMw0+izSh332pm7YFXzGyNu78dI74HgQcBcnJyqm9fRBKs9HAFT+RvZsz5neig0taUEs+DcpjZ1cDFwds3g76ZTqQI6Br1vguRnmDjauPuR78Wm9kzRC5ZHZMgRCRczyzdwv5DFRpSNAWd8BKTmf0CuB1YFbxuD+adSB7Q08x6mFkWMB6YV63NPODmoJppOLDX3beZWTMzaxHsvxlwBbAi7u9KROqEuzNzQSHnd27F4G6tww5Halk8ZxBjgYFBRRNmNhNYChxTthrN3SvMbBrwEpABzHD3lWZ2a7D8AWB+sP0CoAyYHKzeAXgmuC+eCTzm7i+e5PcmIgm24ONdrCs+wG+uHaDS1hQU1yUmoDWwO5huFe/G3X0+kSQQPe+BqGkHboux3npgQLz7EZFwPPxuIWc2y+KqASo+TEXxJIifA0vN7A0iN5UvBu5MaFQikvQ27y7jtTU7mHbpuSptTVEnGjCoAVBFZAzqIUQSxH+7+/Y6iE1Ektg/SluHqbQ1VR03Qbh7lZlNc/e5HHuDWUTSVFl5BU/kbWZ0/450bKXS1lQVz4Nyr5jZt82sq5m1OfpKeGQikrSeWbqFfSptTXnx3IO4JfgafTPZgbNrPxwRSXZHS1v7d27Jhd3PCDscSaB47kHc4e5P1FE8IpLkFn68i492HODXX7xApa0pLp7eXI8pQxWR9JW7oJA2zbL41wFnhR2KJJjuQYhI3DbvLuPV1TuYMLQrjRuqtDXV6R6EiMTt0UUbMTNuVK+taSGe3lx71EUgItGeXbaFn89fTVUS9M87rEcbfvulgWRlpvcIvdv3HuLx9zcxul9HOrVqEnY4UgdqTBBm9h13/1Uwfa27Pxm17Ofu/t26CFDST1WVc8+r62iUmcGoc9uGGktZeQXPLttK06wMfvnv6XtTtvRwBVNm5lFZ5dx+ec+ww5E6crwziPHAr4LpO4kMGnTUaEAJQhLi7wU7Wb+zlLuvG8g1g6qPMVX3urVpyu9fLyC7bTP+45Jzww6nzlVWObfPWcrqbfuYPnEI53U41SFipL45XoKwGqZjvRepNbnvbqBdi0aMPT85OoD71r+cR+GuMn714lq6t2mWdmMu/+z51by6upi7ru7Hpb3bhx2O1KHjXVT1GqZjvRepFRt2lvLG2hKuH9otaa75mxm//uIFXNj9DL41dxlLNn0Sdkh15pGFhcx4dwOTRmYzUU9Np53j/QUOODoGNXBBMH30/fl1FJ+kmVkLC2mYYdwwrFvYoXxK44YZPHjThXRo2Zips/LZvLss7JAS7o21xfxw3kou692e71/VN+xwJAQ1Jgh3z3D3lu7ewt0zg+mj7xvWZZCSHg4cruCp/CLGnt+J9kk4tvGZzRsxY9IQDldUcUtuHnsPHgk7pIRZvW0f02YvoXfHltw7YRAZDXRVOR0lxzm8CPD0kiL2H07uDuDObd+cP914IRt2lnLb7CUcqawKO6RaV7zvEFNy82jeOJPpk3Jo1ijeccUk1ShBSFKoqop0ADegSysGdUvuDuBGntuWn3/hfN4p2MkPnl1BZGDE1FBWXsGUmfnsOXiE6ROH6HmHNKcEIUnhnYKdfFxSyqRR2WGHEpcvDenK1y45h8ff38yDb68PO5xaUVnlfGPOMlZu3cu94wfRv3PcowtLitK5oySFmQsKads8K2lKW+Px/67oxaZdZfzixTV0P7Mpo/vXn9hj+cULq3l51Q5+cFVfLu/bIexwJAnoDEJCt3FXKa+vLeb6Yd3r1djGDRoYv/nSAAZ0ac03nljGB5v3hB3SKZv93kb+/PcN3DyiO5PryVmcJJ4ShIRu1sKNwdjGyVXaGo/GDTP48805tG3eiCkz89my52DYIZ20tz4q4QfPruSSXu34wVV907Y7ETmWEoSEqvRwBXPzNjP2/E50SMLS1ni0a9GIhycN4fCRSm55OI/9h+pP+eva7fu5bfYSerZvzh+uH0xmhv4lyD/pt0FC9fTSLew/XFHvn9Lt2aEFf7xxMAUlB5j22FIq6kH5a/H+Q9ySm0fTrAxmTBpCc5WzSjVKEBKao2MbX9ClFYO7tQ47nNP2mZ7t+Ok1/XnroxJ+9LeVSV3+erC8kq/MzGd3aTnTJw7hrNYqZ5VjKUFIaN4t2EVB8QEmjshOmeveE4Z246sXn82jizYx/Z0NYYcTU1WV8625y1i+ZS/3jB/I+V1UziqxJTRBmNloM1trZgVmdkeM5WZm9wbLl5vZ4GrLM8xsqZk9l8g4JRy5CzbQtnkWVw2o3+Wh1f336N6M7teRn81fzcsrt4cdzjF++dIaXlixnf8Z24cr+nUMOxxJYglLEGaWAdwHjAH6AhPMrHqPX2OAnsFrKnB/teW3A6sTFaOEZ9OuMl5bU8yEod3qVWlrPBo0MH533UAu6NyK2+cs48OivWGH9A+Pv7+JP721nhuGdWPKRRosUo4vkWcQQ4ECd1/v7uXAHGBctTbjgFkesQhobWadAMysC3Al8FACY5SQzFpYGJS2pubYxk2yMvjzxBzaNMtiysw8tiZB+es763byvb+u4OLz2nHX1f1S5rKeJE4iE0RnYHPU+6JgXrxt7ga+Axy3HMTMpppZvpnll5SUnFbAUjdKD1fwRP5mRvfvSMdW9bO0NR7tWzRmxqQhlJVXcktuHgcOV4QWy7od+/na7MWc2645910/SOWsEpdE/pbE+nhSvawjZhszuwoodvfFJ9qJuz/o7jnuntOuXbtTiVPq2DNLt7D/UEVaPLHbq2ML7rthMOuKD/Cfjy0Jpfy1ZP9hJufm0Sgzg+mTcmjRWL31S3wSmSCKgK5R77sAW+NsMwq42swKiVya+pyZPZq4UKWuHC1t7d+5JYOTvNfW2vLZ4JLOG2tL+Mlzq+p034eOVPKVWfnsPHCY6RNz6HJG0zrdv9RviUwQeUBPM+thZlnAeGBetTbzgJuDaqbhwF533+bud7p7F3fPDtZ73d1vTGCsUkcWfLyLdcUHmDSyR1pdA79xeHe+fFEPZi7cyMPv1k35a1WV819zP+CDoj3cfd1ABnRtXSf7ldSRsEcn3b3CzKYBLwEZwAx3X2lmtwbLHwDmA2OBAqAMmJyoeCQ55C4opE2zLK66ILVKW+Nx59g+bNxdxk+eW0W3Nk25rE9ie0z9v5fX8vyH27hzTO9639OshMOS+WnPk5WTk+P5+flhhyE12Ly7jIt//Qa3XXIu3/58r7DDCUVZeQXX/WkRH5ccYO5XRyRszIW5+Zv5zlPLmTC0Kz//wvlpdbYmJ8fMFrt7TqxlKmWQOvPIoo00MOOG4fWv19ba0jQrk4cm5tCqSUOmzMxj+95Dtb6PBQU7+e7TH/KZnm358bj+Sg5yypQgpE6UlVcw5/1NjO7fMe2HsezQMlL+euBQBVNm5lFai+WvBcUHuPXRxfRo24z7bhhMQ5WzymnQb4/Uib8u3cq+QxVMque9ttaWPp1a8ofrB7N62z5un7OUyqrTv9S768BhJue+T1ZmA2ZMGkJLlbPKaVKCkIQ7Wtra76yW5HRPj9LWeFzauz0/urofr64u5qfPn17566EjlUx9ZDHF+w7z55tz6NpG5axy+pQgJOEWrt/F2h37mTgydXptrS03j8hm8qhsHn63kFkLC09pG1VVzv97ajmLN37C764byKA0eb5EEk8jhEjCzVxQyBlNG3L1gLPCDiUpfe/KvmzeXcaP5q2k6xlNubR3+5Na/3evfsTfPtjKd0b3Yuz5KmeV2qMzCEmook/KeGXVDiYM7UbjhqnVa2ttyWhg3DN+EH06tWTaY0tYtXVf3Os+tbiI379ewHU5XfnaZ89JYJSSjpQgJKEeWbQRM+PG4anZa2ttadYok+kTh9CicaT8dce+E5e/Lvx4F3c+vZyR55zJT7+gclapfUoQkjAHyyuZ8/5mPt+vg4a0jEPHVo2ZPimHvQePMGVmHmXlNZe/flwSKWft1qYp999wocpZJSH0WyUJ8+yyLew9eISJI7LDDqXe6HdWK34/YRCrtu7j9jnLYpa/7i4t55bcPDIbGA9PGkqrpipnlcRQgpCEcHdyFxTSp1NLhvZoE3Y49cplfTrw/av68sqqHfzv/E8PqHi4opKvPpLPtr2HePDmHLqdqXJWSRwlCEmI9zbsZs32/Uwa2V3Xxk/B5FE9mDiiOw+9s4FHF20EIkn3O08tJ6/wE35z7QAu1DMlkmAqc5WEyH23kNZNGzJuYPVBBCVe37+qL5t2l/HDeSvp2qYpSzZ+wrPLtvLtK87jX1UyLHVAZxDAvA+2UvRJWdhhpIwtew7y8qrtjB+i0tbTkZnRgN9fP5jzOrTgq4/kc89r6/j3wV247dJzww5N0kTaJ4hPSsv53jMfMiU3n32HjoQdTkp4ZGHkkshNI1TaerqaN8pkxqQczmiaxchzzuR//01dd0vdSfsEcUazLO6/8UI+LjnAtMeWhjJmcCo5dKSSOXmbuKJvRzqrtLVWdGrVhDe+fQmPThlGVmba/8lKHdJvGzDq3Lb87Av9efujEn44byWpNIhSXXt22Rb2lB1h0qjssENJKY0bZtCggc4cpG7pJnXguiHd2LCzjAfe+pgebZvx5c+cHXZI9U6ktHUjvTu2YJhKW0XqPZ1BRPnO53sx9vyO/Gz+al5auT3scOqd9zfsZvW2fUxSr60iKUEJIkqDBsZvvzSQC7q05htzlvFh0d6wQ6pXZi4spFUTlbaKpAoliGoaN8zgoZtzaNMsiykz89i652DYIdULW/cc5KWVOxg/tCtNslTaKpIKlCBiaNeiEQ9PHsLB8kpuyc3jQC2OGZyqHl20EXfnJvXaKpIylCBqcF6HFvzxxsGsKz7AtMeWqPz1OA4dqeTx9zfxL3070OUM9Q0kkiqUII7jMz3b8ZNx/XlzbQk/fm6Vyl9rMO+DrXxSdoSJI7PDDkVEapHKXE/g+mHdKNxVyoNvryf7zGbcclGPsENKKu5O7ruF9OrQghFnnxl2OCJSixJ6BmFmo81srZkVmNkdMZabmd0bLF9uZoOD+Y3N7H0z+8DMVprZXYmM80TuGN2bz/frwE+eX8Wrq3aEGUrSyd/4Cau27WOiSltFUk7CEoSZZQD3AWOAvsAEM+tbrdkYoGfwmgrcH8w/DHzO3QcAA4HRZjY8UbGeSIMGxt3XDeL8zq34+pylrNii8tejct+NlLZeM0i9i4qkmkSeQQwFCtx9vbuXA3OAcdXajANmecQioLWZdQreHwjaNAxeod4AaJIVKX9t3SQyZvC2vSp/3bb3IC+u3M51Q7rSNEtXK0VSTSITRGdgc9T7omBeXG3MLMPMlgHFwCvu/l7iQo1P+5aNmTF5CKWHK5mSm09pmpe/qrRVJLUlMkHEuiBd/SygxjbuXunuA4EuwFAz6x9zJ2ZTzSzfzPJLSkpOJ9649O7Ykj9cP4i1O/bz9ceXxhwzOB1ESls3c1mfDnRto9JWkVSUyARRBHSNet8F2Hqybdx9D/AmMDrWTtz9QXfPcfecdu3anWbI8bmkV3t+dHU/XltTzE+eW1Un+0w2f/tgK7tLy5ms0laRlJXIBJEH9DSzHmaWBYwH5lVrMw+4OahmGg7sdfdtZtbOzFoDmFkT4HJgTQJjPWk3De/OlIt6kLugkJkLCsMOp05Fem0t5LwOzRlxjkpbRVJVwu4sunuFmU0DXgIygBnuvtLMbg2WPwDMB8YCBUAZMDlYvRMwM6iEagDMdffnEhXrqfru2D5s3FXGXX9bSdc2Tfhc7w5hh1QnFm/8hJVb9/GzL/RXaatICrNUejo4JyfH8/Pz63SfZeUVfOlPC9lQUsqTt46k71kt63T/YZj22BLe/qiERd+9TNVLIvWcmS1295xYy9TVxmlqmpXJ9IlDaBmUv+7YdyjskBJq+95DvLBCpa0i6UAJohZ0aNmY6ROHsO/gEabMzKOsPHXLX2e/t5Eqd24anh12KCKSYEoQtaTvWS35w/WDWbV1H19/fFlKlr8eOlLJY+9t4rLeHeh2pkpbRVKdEkQturR3e374r/14dfUOfj5/ddjh1Lrnl29jV2k5k1TaKpIWdBG5lk0cmc2GnaVMf2cD2W2bpcxTxkdLW89t35xR56q0VSQd6AwiAb5/VV8u692eH81byZtri8MOp1Ys2bSHD7fsVa+tImlECSIBMhoY904YRK8OLZj22FLWbN8XdkinLXdBIS0aZ/Jvg6p3pyUiqUoJIkGaNcpk+qQcmjXK4JaH8yiux+WvO/Yd4oUPt/GlnK40a6SrkiLpQgkigTq1asL0iUPYc/AIX56VX2/LX2cv2kilOzePSI37KSISHyWIBOvfuRX3jh/Eii17+eYTy6iqZ+Wvhysqeez9TXyuV3u6n9ks7HBEpA4pQdSBy/t24HtX9uWllTv4xYtJ1efgCT2/fBs7D5QzaVR22KGISB3TBeU6MnlUNoW7Snnw7fVkn9mM64d1CzukEzpa2npOu2ZcdG7bsMMRkTqmM4g6Ymb84Kq+XNKrHd9/dgVvf5T4wY1O19LNe1hetJdJKm0VSUtKEHUoM6MBf7h+MD3bN+e22UtYu31/2CEd18wFhbRolMm/De4SdigiEgIliDrWvFEmMyYNoUlWBrfk5lGy/3DYIcVUvO8Qzy/fxrUqbRVJW0oQITirdaT8dXdpOV+elc/B8sqwQzrG7Pc2qbRVJM0pQYTk/C6tuGf8QJYX7eFbc5Or/LW8oorZ723i0l7tyW6r0laRdKUEEaIr+nXkf8b24YUV2/nVS2vDDucf5n+4jZ0HDjNRvbaKpDVdXA7ZlIt6sGFnKQ+89THZZzZl/NDwy18fXlDI2e2a8RmVtoqkNSWIkJkZd13dj82fHOR//rqC6e9sCDWeKnc+Linlrqv70aCBSltF0pkSRBLIzGjAfdcP4lcvrmVXafhVTTnd23BtjkpbRdKdEkSSaNG4IT+5pn/YYYiI/INuUouISExKECIiEpMShIiIxKQEISIiMSU0QZjZaDNba2YFZnZHjOVmZvcGy5eb2eBgflcze8PMVpvZSjO7PZFxiojIsRKWIMwsA7gPGAP0BSaYWd9qzcYAPYPXVOD+YH4F8F/u3gcYDtwWY10REUmgRJ5BDAUK3H29u5cDc4Bx1dqMA2Z5xCKgtZl1cvdt7r4EwN33A6uBzgmMVUREqklkgugMbI56X8Sx/+RP2MbMsoFBwHuxdmJmU80s38zyS0qSfxAeEZH6IpEPysXqp6F6l6XHbWNmzYG/AN9w932xduLuDwIPBu1LzGzjqYWbNNoCO8MOIknoWHyajsen6Xj80+kcixr79E9kgigCuka97wJsjbeNmTUkkhxmu/vT8ezQ3dudcrRJwszy3T0n7DiSgY7Fp+l4fJqOxz8l6lgk8hJTHtDTzHqYWRYwHphXrc084Oagmmk4sNfdt1lkAOTpwGp3/20CYxQRkRok7AzC3SvMbBrwEpABzHD3lWZ2a7D8AWA+MBYoAMqAycHqo4CbgA/NbFkw77vuPj9R8YqIyKcltLO+4B/6/GrzHoiaduC2GOu9Q+z7E+ngwbADSCI6Fp+m4/FpOh7/lJBjYZH/0SIiIp+mrjZERCQmJQgREYlJCSIJqO+pY5lZhpktNbPnwo4lbGbW2syeMrM1we/IiLBjCpOZfTP4O1lhZo+bWeOwY6pLZjbDzIrNbEXUvDZm9oqZrQu+nlEb+1KCSA7qe+pYtxPpYkXgHuBFd+8NDCCNj4uZdQa+DuS4e38iFZLjw42qzuUCo6vNuwN4zd17Aq8F70+bEkQSUN9Tn2ZmXYArgYfCjiVsZtYSuJjIc0G4e7m77wk1qPBlAk3MLBNoyrEP4KY0d38b2F1t9jhgZjA9E7imNvalBJFkTtT3VJq4G/gOUBVyHMngbKAEeDi45PaQmTULO6iwuPsW4P+ATcA2Ig/XvhxuVEmhg7tvg8gHTqB9bWxUCSKJxNP3VKozs6uAYndfHHYsSSITGAzc7+6DgFJq6fJBfRRcWx8H9ADOApqZ2Y3hRpW6lCCSxKn0PZWiRgFXm1khkS7iP2dmj4YbUqiKgCJ3P3pG+RSRhJGuLgc2uHuJux8BngZGhhxTMthhZp0Agq/FtbFRJYgkoL6n/snd73T3Lu6eTeTm4+vunrafEN19O7DZzHoFsy4DVoUYUtg2AcPNrGnwd3MZaXzTPso8YGIwPRF4tjY2mtCuNiRu6ntKjuc/gdlBp5fr+WefZWnH3d8zs6eAJUSq/5aSZl1umNnjwCVAWzMrAn4I/AKYa2ZTiCTRa2tlX+pqQ0REYtElJhERiUkJQkREYlKCEBGRmJQgREQkJiUIERGJSQlCTpmZuZn9Jur9t83sR7W07Vwz+2JtbOsE+7k26CH1jUTv61SY2UAzG1uL23voRB1B1nTszewS9a6bXpQg5HQcBv7NzNqGHUg0M8s4ieZTgP9w90sTFU+0oIO5kzGQyLjttcLdv+zuSfWg3SkcE6kjShByOiqIPKT0zeoLqn8KNbMDwddLzOwtM5trZh+Z2S/M7AYze9/MPjSzc6I2c7mZ/T1od1WwfoaZ/drM8sxsuZl9NWq7b5jZY8CHMeKZEGx/hZn9Mpj3A+Ai4AEz+3W19peY2ZtR4zDMDp7cxcwuDL6HxWb2UlQXB18J4vrAzP5iZk2jjsVvg7OUX5rZOWb2YrD+382sd9Du2iC+D8zs7eDBuB8D15nZMjO7rlqMk8zs6WBb68zsV1HLrjCzhWa2xMyeDPr5IviecoLpKcGxfdPM/mxmf4ja/MVmtsDM1lc7m2hpZs+Y2Soze8DMGtR0fKN/7sH0F80st4Zj8tnge1xmkU4JW1T/GUoI3F0vvU7pBRwAWgKFQCvg28CPgmW5wBej2wZfLwH2AJ2ARsAW4K5g2e3A3VHrv0jkQ0xPIn0SNQamAt8L2jQC8ol03HYJkY7sesSI8ywiT5e2I9J7wOvANcGyN4mMLVB9nUuAvUCXIIaFRJJJQ2AB0C5odx0wI5g+M2r9nwL/GfW9PAdkBO9fA3oG08OIdCcCkcTWOZhuHXydBPyhhuM/iciT1a2CY7MR6Aq0Bd4GmgXt/hv4QfT3GxyTQqBN8D39/eh+gnifDL7vvkBB1DE5RKSH2QzgFeCLJzi+B6Li/SKQW8Mx+RswKphuDmSG/futl6urDTk97r7PzGYRGcTlYJyr5XnQNbGZfQwc7a75QyD6Us9cd68C1pnZeqA3cAVwQdSn2lZEEkg58L67b4ixvyHAm+5eEuxzNpExFv56gjjfd/eiYJ1lQDaR5NYfeCU4ocgg0u00QH8z+ynQmsg/uZeitvWku1cGn+RHAk8G60Mk0QG8C+Sa2VwindDF4zV33xvEuAroHuy/L/BusI8sIgku2lDgLXffHaz7JHBe1PK/Bsd+lZl1qHZM1gfrPE4kaR7h1I7vk+5eGUy/C/w2WPfpo8ddwqUEIbXhbiJ94zwcNa+C4BJmcGkmK2rZ4ajpqqj3VXz6d7J6PzAOGJFP5tH/fDGzS4icQcRiNcw/keg4K4PYDFjp7rGG/cwl8sn5AzObROQT91FHY2sA7HH3gdVXdvdbzWwYkcGSlpnZMW1OIsZX3H3CcdY70TGJ3m5025p+JjWJbl99aNB//Lzc/Rdm9jyR+y2LzOxyd19zghglwXQPQk5b8Cl0LpEbvkcVAhcG0+OIXMY4WdeaWYPgvsTZwFoin8q/ZpHu0TGz8+zEA+i8B3zWzNpa5Ab2BOCtU4iHIIZ2FowLbWYNzaxfsKwFsC2I7YZYK3tknI8NZnZtsL6Z2YBg+hx3f8/dfwDsJHK5aH+w3ZOxCBhlZucG221qZudVa/M+kWNyhkVuEv97nNseamY9gnsP1wHvcPzju8PM+gTtv1DTRoPv/UN3/yWRy4a944xHEkgJQmrLb4hc+z7qz0T+abxP5Dp7TZ/uj2ctkX80LwC3uvshIsOQrgKWWGTQ9j9xgjPh4HLWncAbwAfAEnc/pe6Q3b2cyLX0X5rZB8Ay/jkewfeJ/LN8BTjep98bgCnB+iuJJFCAXx+90UvkHsIHQcx9Y92kPk6MJUTuTzxuZsuJJIze1dpsAX4exPsqkWO6N47NLyTSc+gKYAPwzAmO7x1E7jW8zj8vxcXyjaM36IlcqnwhjlgkwdSbq0iaMrPm7n4gOIN4hsjN9mfCjkuSh84gRNLXj4Kb70fPBv4aajSSdHQGISIiMekMQkREYlKCEBGRmJQgREQkJiUIERGJSQlCRERi+v92s9LMD0WeUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(10)+1,error_rate2)\n",
    "plt.xlabel('Number of nearest neighbours')\n",
    "plt.ylabel('Error rate')\n",
    "\n",
    "#The Lowest error rate seems to occur in K = [2,3] suggestting that the optimal k is in that range. It most also be noted that\n",
    "#the plot dosen't contain a nice smooth U-shape expected. This is due to test set not being large enough to smoth out all the\n",
    "#'noise' in the data. So, the prediction for optimal K may not be entirely correct. To overcome this a lrager dataset and/or\n",
    "#cross validation should be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63fe1ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10606060606060606, 0.007575757575757576]\n",
      "[0.003787878787878788, 0.4090909090909091]\n",
      "[0.25, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.9545454545454546]\n",
      "[0.16666666666666666, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.3712121212121212]\n",
      "[0.03409090909090909, 0.007575757575757576]\n",
      "[0.003787878787878788, 0.8712121212121212]\n",
      "[0.003787878787878788, 0.9734848484848485]\n",
      "[0.003787878787878788, 0.5795454545454546]\n",
      "[0.11742424242424243, 0.003787878787878788]\n",
      "[0.2765151515151515, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.4734848484848485]\n",
      "[0.2727272727272727, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.3939393939393939]\n",
      "[0.003787878787878788, 0.3560606060606061]\n",
      "[0.07954545454545454, 0.003787878787878788]\n",
      "[0.2727272727272727, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.5454545454545454]\n",
      "[0.8257575757575758, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.5454545454545454]\n",
      "[0.003787878787878788, 0.8068181818181818]\n",
      "[0.13636363636363635, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.8068181818181818]\n",
      "[0.17045454545454544, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.42424242424242425]\n",
      "[0.003787878787878788, 0.803030303030303]\n",
      "[0.2840909090909091, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.3977272727272727]\n",
      "[0.003787878787878788, 0.8181818181818182]\n",
      "[0.2537878787878788, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.928030303030303]\n",
      "[0.003787878787878788, 0.9507575757575758]\n",
      "[0.003787878787878788, 0.9166666666666666]\n",
      "[0.003787878787878788, 0.625]\n",
      "[0.003787878787878788, 0.04924242424242424]\n",
      "[0.29924242424242425, 0.003787878787878788]\n",
      "[0.1856060606060606, 0.003787878787878788]\n",
      "[0.11742424242424243, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.5189393939393939]\n",
      "[0.003787878787878788, 0.7310606060606061]\n",
      "[0.20075757575757575, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.8446969696969697]\n",
      "[0.003787878787878788, 0.803030303030303]\n",
      "[0.003787878787878788, 0.9507575757575758]\n",
      "[0.20075757575757575, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.7348484848484849]\n",
      "[0.003787878787878788, 0.3181818181818182]\n",
      "[0.003787878787878788, 0.7537878787878788]\n",
      "[0.003787878787878788, 0.7803030303030303]\n",
      "[0.007575757575757576, 0.026515151515151516]\n",
      "[0.003787878787878788, 0.4053030303030303]\n",
      "[0.2537878787878788, 0.003787878787878788]\n",
      "[0.17045454545454544, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.9696969696969697]\n",
      "[0.003787878787878788, 0.3106060606060606]\n",
      "[0.16666666666666666, 0.003787878787878788]\n",
      "[0.03787878787878788, 0.007575757575757576]\n",
      "[0.003787878787878788, 0.5454545454545454]\n",
      "[0.29924242424242425, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.45075757575757575]\n",
      "[0.003787878787878788, 0.9015151515151515]\n",
      "[0.003787878787878788, 0.7386363636363636]\n",
      "[0.003787878787878788, 0.8522727272727273]\n",
      "[0.003787878787878788, 0.8484848484848485]\n",
      "[0.003787878787878788, 0.5681818181818182]\n",
      "[0.12121212121212122, 0.003787878787878788]\n",
      "[0.20075757575757575, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.9545454545454546]\n",
      "[0.003787878787878788, 0.6893939393939394]\n",
      "[0.003787878787878788, 0.8068181818181818]\n",
      "[0.003787878787878788, 0.29545454545454547]\n",
      "[0.003787878787878788, 0.8939393939393939]\n",
      "[0.003787878787878788, 0.4393939393939394]\n",
      "[0.10606060606060606, 0.003787878787878788]\n",
      "[0.01893939393939394, 0.01893939393939394]\n",
      "[0.003787878787878788, 0.6439393939393939]\n",
      "[0.003787878787878788, 0.9090909090909091]\n",
      "[0.003787878787878788, 0.4734848484848485]\n",
      "[0.003787878787878788, 0.3560606060606061]\n",
      "[0.003787878787878788, 0.8143939393939394]\n",
      "[0.003787878787878788, 0.5492424242424242]\n",
      "[0.003787878787878788, 0.4393939393939394]\n",
      "[0.003787878787878788, 0.9242424242424242]\n",
      "[0.11363636363636363, 0.003787878787878788]\n",
      "[0.21212121212121213, 0.003787878787878788]\n",
      "[0.09848484848484848, 0.003787878787878788]\n",
      "[0.003787878787878788, 0.8560606060606061]\n",
      "306.3942856788635 seconds\n"
     ]
    }
   ],
   "source": [
    "#get p-values for the X_train set\n",
    "start = time.time()\n",
    "\n",
    "p_vals = list(np.zeros([X_test.shape[0]]))\n",
    "\n",
    "for i in range(len(p_vals)):\n",
    "    p_vals[i] = conformal_predictor(X_test[i], X_train, y_train)\n",
    "    print(p_vals[i])\n",
    "\n",
    "print(time.time() - start, 'seconds')\n",
    "###As can be seen from my output obtaining p values for all labels for all samples in X_test takes a long time (around 5mins). \n",
    "###This is understandable given the large number of features however there is likely still much room for efficency improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb882a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average false p_value is 0.004863980716253442.\n"
     ]
    }
   ],
   "source": [
    "#Get false p-value \n",
    "unique_labels = np.unique(y_train)\n",
    "false_p_vals = np.zeros(y_test.size)\n",
    "\n",
    "for i in range(y_test.size):\n",
    "    lab_of_index = y_test[i]\n",
    "    samp_of_index_p = p_vals[i]\n",
    "    diff_lab = unique_labels[unique_labels != lab_of_index]\n",
    "    if diff_lab == -1:\n",
    "        false_p_vals[i] = samp_of_index_p[0]\n",
    "    else:\n",
    "        false_p_vals[i] = samp_of_index_p[1]\n",
    "\n",
    "avg_false_p_value = np.mean(false_p_vals)\n",
    "\n",
    "print('The average false p_value is ' + str(avg_false_p_value) + '.')\n",
    "\n",
    "#The average false p value is approximatly 0.004864. The lowest possible p value is 1/(n +1) = 1/(263 + 1) = 0.003788 (4sf). \n",
    "# Given the average false p value is near to lowest possible p value, the 1NN model can be seen as a good fit for the \n",
    "#ionoshpere dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
